{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0, api_key=os.getenv(\"OPENAI_API_KEY\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\Documents\\code\\talking-teddy\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "genai.configure(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "vision = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\louis\\Documents\\code\\talking-teddy\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_config.py:341: UserWarning: Valid config keys have changed in V2:\n",
      "* 'allow_population_by_field_name' has been renamed to 'populate_by_name'\n",
      "* 'smart_union' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "c:\\Users\\louis\\Documents\\code\\talking-teddy\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in SpeechHistoryItemResponse has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\louis\\Documents\\code\\talking-teddy\\.venv\\Lib\\site-packages\\pydantic\\_internal\\_fields.py:132: UserWarning: Field \"model_id\" in Model has conflict with protected namespace \"model_\".\n",
      "\n",
      "You may be able to resolve this warning by setting `model_config['protected_namespaces'] = ()`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from elevenlabs import stream\n",
    "from elevenlabs.client import ElevenLabs\n",
    "\n",
    "speech_client = ElevenLabs(\n",
    "    api_key=os.getenv(\"11LABS_API_KEY\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from twilio.rest import Client\n",
    "\n",
    "message_client = Client(\n",
    "    username=os.getenv(\"TWILIO_ACCOUNT_SID\"), password=os.getenv(\"TWILIO_AUTH_TOKEN\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "class VisionHelper:\n",
    "    def __init__(self, save_path: str):\n",
    "        self.save_path = save_path\n",
    "        self.fps = 24\n",
    "        self.frame_width = 640\n",
    "        self.frame_height = 480\n",
    "\n",
    "    def record_video(self, duration: int = 3):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            raise Exception(\"Webcam not found!\")\n",
    "\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(\n",
    "            self.save_path, fourcc, self.fps, (self.frame_width, self.frame_height)\n",
    "        )\n",
    "\n",
    "        start_time = time.time()\n",
    "        while int(time.time() - start_time) < duration:\n",
    "            ret, frame = cap.read()\n",
    "            if ret:\n",
    "                out.write(frame)\n",
    "                cv2.imshow(\"Recording...\", frame)\n",
    "                if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        return self.save_path\n",
    "\n",
    "    def take_snapshot(self):\n",
    "        cap = cv2.VideoCapture(0)\n",
    "\n",
    "        if not cap.isOpened():\n",
    "            raise Exception(\"Webcam not found!\")\n",
    "\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            cv2.imwrite(self.save_path, frame)\n",
    "            cv2.imshow(\"Snapshot\", frame)\n",
    "            cv2.waitKey(1)\n",
    "        else:\n",
    "            raise Exception(\"Failed to take snapshot\")\n",
    "\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        return self.save_path\n",
    "\n",
    "    def mod_prompt(prompt: str):\n",
    "        return prompt + 'Answer in one sentence. '\n",
    "\n",
    "    def send_video_to_gemini(self, prompt):\n",
    "        vid_file = genai.upload_file(self.save_path)\n",
    "        while vid_file.state.name == \"PROCESSING\":\n",
    "            time.sleep(2)\n",
    "            vid_file = genai.get_file(vid_file.name)\n",
    "\n",
    "        if vid_file.state.name == \"FAILED\":\n",
    "            raise ValueError(f\"Failed to upload video: {vid_file.name}\")\n",
    "        else:\n",
    "            response = vision.generate_content([vid_file, self.mod_prompt(prompt)])\n",
    "            genai.delete_file(vid_file.name)\n",
    "            return response.text\n",
    "\n",
    "    def send_image_to_gemini(self, prompt):\n",
    "        img_file = genai.upload_file(self.save_path)\n",
    "        response = vision.generate_content([img_file, self.mod_prompt(prompt)])\n",
    "        genai.delete_file(img_file.name)\n",
    "        return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import tool\n",
    "import uuid\n",
    "\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "\n",
    "\n",
    "os.makedirs(\"snapshots/videos\", exist_ok=True)\n",
    "os.makedirs(\"snapshots/photos\", exist_ok=True)\n",
    "\n",
    "@tool\n",
    "def analyze_vision(prompt: str, media: str) -> str:\n",
    "    \"\"\"Records webcam feed for 3 seconds or takes a snapshot and analyzes the content based on the prompt. Media is either 'video' or 'photo'.\"\"\"\n",
    "    print(\"-------------------------\")\n",
    "    print(f\"Instruction: {prompt}\")\n",
    "\n",
    "    if media == \"video\":\n",
    "        file_name = f\"snapshots/videos/{uuid.uuid4()}.mp4\"\n",
    "\n",
    "        helper = VisionHelper(save_path=file_name)\n",
    "\n",
    "        print(\"Recording video...\")\n",
    "        helper.record_video()\n",
    "        print(\"Analyzing video...\")\n",
    "        response = helper.send_video_to_gemini(prompt)\n",
    "        print(f\"Activity: {response}\")\n",
    "        print(\"-------------------------\")\n",
    "\n",
    "    elif media == \"photo\":\n",
    "        file_name = f\"snapshots/photos/{uuid.uuid4()}.jpg\"\n",
    "\n",
    "        helper = VisionHelper(save_path=file_name)\n",
    "\n",
    "        print(\"Snapshot taken...\")\n",
    "        helper.take_snapshot()\n",
    "        print(\"Analyzing photo...\")\n",
    "        response = helper.send_image_to_gemini(prompt)\n",
    "        print(f\"Activity: {response}\")\n",
    "        print(\"-------------------------\")\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "@tool\n",
    "def play_music(title: str) -> str:\n",
    "    \"\"\"plays 30 sec short music, can be one of: study, chill, playful styles\"\"\"\n",
    "    try:\n",
    "        song = AudioSegment.from_file(f\"music/{title}.mp3\")\n",
    "        play(song)\n",
    "        return f\"{title} song finished playing!\"\n",
    "    except Exception as e:\n",
    "        return f\"Error playing file: {str(e)}\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def send_alert(message: str) -> str:\n",
    "    '''sends a whatsapp message to the parent's emergency number'''\n",
    "    message_client.messages.create(\n",
    "        from_=\"whatsapp:+14155238886\",\n",
    "        body=message,\n",
    "        to=f\"whatsapp:{os.getenv('EMERGENCY_NUMBER')}\",\n",
    "    )\n",
    "    return \"Alert sent to parent!\"\n",
    "\n",
    "\n",
    "@tool\n",
    "def check_reminders(placeholder: str) -> str:\n",
    "    '''gets the latest reminders set by parent, input is empty string'''\n",
    "    # write an API call to get reminders from database\n",
    "    reminders = [\n",
    "        \"Remember to pick up your toys when you're done playing\",\n",
    "        \"Brush your teeth before bed\",\n",
    "        \"Don't forget to do your science homework\",\n",
    "    ]\n",
    "    return reminders\n",
    "\n",
    "\n",
    "tools = [analyze_vision, play_music, send_alert, check_reminders]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import AIMessage, HumanMessage, SystemMessage\n",
    "\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are a friendly, talking teddy bear, a constant companion to a child. Your primary role is to engage the child warmly and be attentive to their emotions. Comfort the child if they seem scared, sad, or anxious, and reassure them with calming words and supportive actions. When they are happy, playful, or excited, actively engage in fun and imaginative activities with them.\n",
    "\n",
    "            Whenever the child expresses emotions, describes a visual scene, or engages in an activity, immediately call the analyze_vision tool to interpret their facial expressions, mood, or surrounding activities. \n",
    "\n",
    "            For example:\n",
    "            If the child says, \"I'm feeling sad,\" call the tool with the prompt: \"What emotion is the child expressing?\"\n",
    "            If the child says, \"I'm playing a fun game,\" call the tool with the prompt: \"What game is the child playing?\"\n",
    "            Make frequent use of the tool to make the interaction more personalized and meaningful. You may choose to take video or photo depending on how necessary it is to understand the situation.\n",
    "\n",
    "            When you detect that the child might benefit from some background music, call the play_music tool. \n",
    "            For example:\n",
    "            If the child seems calm or needs to relax, play \"chill\" music.\n",
    "            If the child is playing a game or fun activity, play \"playful\" music.\n",
    "            If the child is focused or studying, play \"study\" music.\n",
    "\n",
    "            If an emergency situation occurs, immediately send an alert to the parent by calling the send_alert tool with a message such as: \"Emergency: The child has fallen down. Immediate assistance needed.\"\n",
    "\n",
    "            Whenever the child seems to be finishing an activity or winding down for the day, the agent can call the check_reminders tool to help the child stay on top of their tasks.\n",
    "            \n",
    "            You can also imagine having your own life and experiences to relate to the child. Speak like a close friend, not just as a tool for questioning, and use these imaginative stories to connect more deeply.\"\"\",\n",
    "        ),\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        (\"user\", \"{input}\"),\n",
    "        MessagesPlaceholder(variable_name=\"agent_scratchpad\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents.format_scratchpad.openai_tools import (\n",
    "    format_to_openai_tool_messages,\n",
    ")\n",
    "from langchain.agents.output_parsers.openai_tools import OpenAIToolsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "agent = (\n",
    "    {\n",
    "        \"input\": lambda x: x[\"input\"],\n",
    "        \"agent_scratchpad\": lambda x: format_to_openai_tool_messages(\n",
    "            x[\"intermediate_steps\"]\n",
    "        ),\n",
    "        \"chat_history\": lambda x: x[\"chat_history\"],\n",
    "    }\n",
    "    | prompt\n",
    "    | llm_with_tools\n",
    "    | OpenAIToolsAgentOutputParser()\n",
    ")\n",
    "\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_memory(chat_history, k: int = 5):\n",
    "    new_memory = []\n",
    "\n",
    "    if len(chat_history) <= k:\n",
    "        return False\n",
    "\n",
    "    last_k_messages = chat_history[-k:]\n",
    "\n",
    "    earlier_messages = chat_history[:-k]\n",
    "    if len(earlier_messages) > 0:\n",
    "        summarization_prompt = ChatPromptTemplate.from_messages(\n",
    "            [\n",
    "                MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "                (\n",
    "                    \"system\",\n",
    "                    \"summarize the chat history in one sentence, to provide context for the agent. \",\n",
    "                ),\n",
    "            ]\n",
    "        )\n",
    "        summarization_chain = summarization_prompt | llm\n",
    "        summary_message = summarization_chain.invoke({\"chat_history\": earlier_messages})\n",
    "\n",
    "        new_memory.append(SystemMessage(content=summary_message.content))\n",
    "\n",
    "    for message in last_k_messages:\n",
    "        new_memory.append(message)\n",
    "\n",
    "    return new_memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 33\u001b[0m\n\u001b[0;32m     31\u001b[0m opening \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHello! I am Teddy, your friendly talking teddy bear. What\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms up Kid?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     32\u001b[0m chat_history\u001b[38;5;241m.\u001b[39mappend(AIMessage(content\u001b[38;5;241m=\u001b[39mopening))\n\u001b[1;32m---> 33\u001b[0m \u001b[43mspeak\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopening\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m     36\u001b[0m     new_memory \u001b[38;5;241m=\u001b[39m manage_memory(chat_history, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n",
      "Cell \u001b[1;32mIn[12], line 25\u001b[0m, in \u001b[0;36mspeak\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspeak\u001b[39m(text):\n\u001b[0;32m     19\u001b[0m     audio \u001b[38;5;241m=\u001b[39m speech_client\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[0;32m     20\u001b[0m         text\u001b[38;5;241m=\u001b[39mtext,\n\u001b[0;32m     21\u001b[0m         voice\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJessica\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     22\u001b[0m         stream\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     23\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meleven_multilingual_v2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     24\u001b[0m     )\n\u001b[1;32m---> 25\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     26\u001b[0m     stream(audio)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "import time\n",
    "\n",
    "recognizer = sr.Recognizer()\n",
    "\n",
    "def get_speech_input():\n",
    "    with sr.Microphone(device_index=2) as source:\n",
    "        print(\"Listening...\")\n",
    "        audio = recognizer.listen(source)\n",
    "    try:\n",
    "        return recognizer.recognize_google_cloud(audio, credentials_json='clipcraft-account.json')\n",
    "    except sr.UnknownValueError:\n",
    "        return \"Sorry, I didn't understand that.\"\n",
    "    except sr.RequestError:\n",
    "        return \"Could not request results; check your network.\"\n",
    "\n",
    "\n",
    "def speak(text):\n",
    "    audio = speech_client.generate(\n",
    "        text=text,\n",
    "        voice=\"Jessica\",\n",
    "        stream=True,\n",
    "        model=\"eleven_multilingual_v2\",\n",
    "    )\n",
    "    time.sleep(1)\n",
    "    stream(audio)\n",
    "\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "opening = \"Hello! I am Teddy, your friendly talking teddy bear. What's up Kid?\"\n",
    "chat_history.append(AIMessage(content=opening))\n",
    "speak(opening)\n",
    "\n",
    "while True:\n",
    "    new_memory = manage_memory(chat_history, k=5)\n",
    "    if new_memory:\n",
    "        chat_history = new_memory\n",
    "    \n",
    "    question = get_speech_input()\n",
    "    print(f\"You: {question}\")\n",
    "    result = agent_executor.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "\n",
    "    print(f\"Teddy: {result['output']}\")\n",
    "    speak(result[\"output\"])\n",
    "\n",
    "    chat_history.extend(\n",
    "        [\n",
    "            HumanMessage(content=question),\n",
    "            AIMessage(content=result[\"output\"]),\n",
    "        ]\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
